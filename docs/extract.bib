@article{Acerbi2002b,
 abstract = {1000-2500 citations},
 author = {Acerbi, Carlo},
 doi = {10.1016/S0378-4266(02)00281-9},
 file = {:C\:/S/Library/Acerbi/2002_Spectral measures of risk A coherent representation of subjective risk aversion.pdf:pdf},
 issn = {03784266},
 journal = {Journal of Banking & Finance},
 keywords = {GSC,Risk Measures,coherence,conditional value-at-risk,expected shortfall,quan-,risk measure,value-at-risk},
 mendeley-tags = {GSC,Risk Measures},
 month = {jul},
 number = {7},
 pages = {1505--1518},
 title = {{Spectral measures of risk: A coherent representation of subjective risk aversion}},
 url = {http://linkinghub.elsevier.com/retrieve/pii/S0378426602002819},
 volume = {26},
 year = {2002}
}

@article{Anderson1988,
 abstract = {In recent years catastrophe reinsurers' use of catastrophe models has been increasing until currently virtually all of the catastrophe reinsurers in the world use a catastrophe model to aid them in their pricing and portfolio management decisions. This paper explicitly models various types of reinstatement provisions, including reinstatements that are limited by the number of occurrences and by the aggregate losses; and reinstatement premiums based on the size of loss and by the time elapsed to the first occurrence. The paper also investigates the effects on the fair premium of a catastrophe treaty when various reinstatement provisions are considered. This is an expansion of the methods developed in papers by Leroy J. Simon and Bjom Sundt, which were written before the widespread use of catastrophe models. The catastrophe model used for this paper is the Insurance / Investment Risk Assessment System (IRAS) produced by Risk Management Solutions, Inc.},
 author = {Anderson, RR and Dong, Wemin},
 file = {:C\:/S/Library/Anderson, Dong/1988_Pricing catastrophe reinsurance with reinstatement provisions using a catastrophe model.pdf:pdf},
 journal = {Casualty Actuarial Society Forum},
 keywords = {FFT},
 mendeley-tags = {FFT},
 pages = {303--322},
 title = {{Pricing catastrophe reinsurance with reinstatement provisions using a catastrophe model}},
 year = {1988}
}

@techreport{AonBenfield2015f,
 author = {{Aon Benfield}},
 edition = {10},
 file = {:C\:/S/Library/Aon Benfield/2015_Insurance Risk Study, Ed. 10.pdf:pdf},
 institution = {Aon Limited},
 keywords = {Aon,Insurance Risk Study},
 mendeley-tags = {Aon,Insurance Risk Study},
 publisher = {Aon Limited},
 title = {{Insurance Risk Study, Ed. 10}},
 year = {2015}
}

@article{Arratia2019,
 abstract = {Size bias occurs famously in waiting-time paradoxes, undesirably in sampling schemes, and unexpectedly in connection with Stein's method, tightness, analysis of the lognormal distribution, Skorohod embedding, infinite divisibility, branching processes, and number theory. In this paper we review the basics and survey some of these unexpected connections.},
 archiveprefix = {arXiv},
 arxivid = {1308.2729},
 author = {Arratia, Richard and Goldstein, Larry and Kochman, Fred},
 doi = {10.1214/13-PS221},
 eprint = {1308.2729},
 file = {:C\:/S/Library/Arratia, Goldstein, Kochman/2019_Size bias for one and all.pdf:pdf},
 issn = {15495787},
 journal = {Probability Surveys},
 keywords = {FFT},
 mendeley-tags = {FFT},
 pages = {1--61},
 title = {{Size bias for one and all}},
 volume = {16},
 year = {2019}
}

@article{Bear1990,
 author = {Bear, R.A. and Nemlick, K.J.},
 doi = {10.1016/0167-6687(93)91078-9},
 file = {:C\:/S/Library/Bear, Nemlick/1990_Pricing the impact of adjustable features and loss sharing provisions of reinsurance treaties.pdf:pdf},
 issn = {01676687},
 journal = {Proceedings of the Casualty Actuarial Society},
 keywords = {FFT},
 mendeley-tags = {FFT},
 number = {147},
 pages = {86--87},
 title = {{Pricing the impact of adjustable features and loss sharing provisions of reinsurance treaties}},
 volume = {77},
 year = {1990}
}

@article{Bernegger1997,
 author = {Bernegger, Stefan},
 file = {:C\:/S/Library/Bernegger/1997_The Swiss Re exposure curves and the MBBEFD distribution class.pdf:pdf},
 journal = {ASTIN Bulletin},
 keywords = {FFT},
 mendeley-tags = {FFT},
 number = {1},
 pages = {99--111},
 title = {{The Swiss Re exposure curves and the MBBEFD distribution class}},
 volume = {27},
 year = {1997}
}

@book{billingsley,
 author = {Billingsley, Patrick},
 edition = {Second},
 publisher = {J. Wiley and Sons},
 title = {{Probability and Measure}},
 year = {1986}
}

@article{Bodoff2007,
 author = {Bodoff, Neil M.},
 file = {:C\:/S/Library/Bodoff/2007_Capital Allocation by Percentile Layer.pdf:pdf},
 journal = {Variance},
 keywords = {capital allocation,enterprise risk,management,percentile layer of capital,risk load,risk-adjusted profitability,value at risk},
 number = {1},
 pages = {13--30},
 title = {{Capital Allocation by Percentile Layer}},
 volume = {3},
 year = {2007}
}

@article{Boonen2017,
 abstract = {Existing risk capital allocation methods, such as the Euler rule, work under the explicit assumption that portfolios are formed as linear combinations of random loss/profit variables, with the firm being able to choose the portfolio weights. This assumption is unrealistic in an insurance context, where arbitrary scaling of risks is generally not possible. Here, we model risks as being partially generated by L{\'{e}}vy processes, capturing the non-linear aggregation of risk. The model leads to non-homogeneous fuzzy games, for which the Euler rule is not applicable. For such games, we seek capital allocations that are in the core, that is, do not provide incentives for splitting portfolios. We show that the Euler rule of an auxiliary linearised fuzzy game (non-uniquely) satisfies the core property and, thus, provides a plausible and easily implemented capital allocation. In contrast, the Aumann–Shapley allocation does not generally belong to the core. For the non-homogeneous fuzzy games studied, Tasche's (1999) criterion of suitability for performance measurement is adapted and it is shown that the proposed allocation method gives appropriate signals for improving the portfolio underwriting profit.},
 author = {Boonen, Tim J. and Tsanakas, Andreas and W{\"{u}}thrich, Mario V.},
 doi = {10.1016/j.insmatheco.2016.11.003},
 file = {:C\:/S/Library/Boonen, Tsanakas, W{\"{u}}thrich/2017_Capital allocation for portfolios with non-linear risk aggregation(2).pdf:pdf},
 issn = {01676687},
 journal = {Insurance: Mathematics and Economics},
 keywords = {Aumann–Shapley value,Capital allocation,Euler rule,Fuzzy core,Risk measures,Tsanakas},
 mendeley-tags = {Tsanakas},
 pages = {95--106},
 title = {{Capital allocation for portfolios with non-linear risk aggregation}},
 volume = {72},
 year = {2017}
}

@book{Borwein2010,
 author = {Borwein, Jonathan M and Vanderwerff, Jon D},
 file = {:C\:/S/Library/Borwein, Vanderwerff/2010_Convex Functions - Construction, Characterizations and Counterexamples.pdf:pdf},
 isbn = {9780521850056},
 publisher = {Cambridge University Press},
 title = {{Convex Functions - Construction, Characterizations and Counterexamples}},
 year = {2010}
}

@book{Bowers1997,
 author = {Bowers, Newton and Gerber, Hans and Hickman, James and Jones, Donald and Nesbitt, Cecil},
 doi = {10.2307/253313},
 file = {:C\:/S/Library/Bowers et al/1997_Actuarial Mathematics.pdf:pdf},
 issn = {00224367},
 keywords = {Book},
 mendeley-tags = {Book},
 publisher = {Society of Actuaries},
 title = {{Actuarial Mathematics}},
 year = {1997}
}

@article{Buhlmann1984,
 author = {B{\"{u}}hlmann, Hans},
 doi = {10.1080/03461238.1984.10413759},
 file = {:C\:/S/Library/B{\"{u}}hlmann/1984_Numerical evaluation of the compound Poisson distribution Recursion or fast fourier transform.pdf:pdf},
 issn = {16512030},
 journal = {Scandinavian Actuarial Journal},
 keywords = {FFT},
 mendeley-tags = {FFT},
 number = {2},
 pages = {116--126},
 title = {{Numerical evaluation of the compound Poisson distribution: Recursion or fast fourier transform?}},
 volume = {1984},
 year = {1984}
}

@article{Butsic1994,
 abstract = {Regulators have recently adopted a risk-based capital formula for property-liability insurers. This article develops practical methods for setting risk-based capital standards using the expected policyholder deficit as the solvency measure. The analysis considers the stochastic nature of insurance risk, using market valuation to remove measurement bias, and finds that a proper time horizon is the period between risk-based capital evaluations. The present value of the expected policyholder deficit is shown to be equivalent to a financial option implicitly given by the policyholders. Finally, covariance of risk elements is considered, deriving a simple square root rule.},
 author = {Butsic, Robert P},
 file = {:C\:/S/Library/Butsic/1994_Solvency Measurement for Property-Liability Risk-Based Capital Applications.pdf:pdf},
 isbn = {00224367},
 journal = {The Journal of Risk and Insurance},
 keywords = {Risk Measures},
 mendeley-tags = {Risk Measures},
 number = {4},
 pages = {656--690},
 pmid = {9502081283},
 title = {{Solvency Measurement for Property-Liability Risk-Based Capital Applications}},
 url = {http://www.jstor.org/stable/253643%5Cnhttp://www.jstor.org/page/},
 volume = {61},
 year = {1994}
}

@article{Campi2013,
 abstract = {In this article, we characterize efficient portfolios, i.e. portfolios which are optimal for at least one rational agent, in a very general multi-currency financial market model with proportional transaction costs. In our setting, transaction costs may be random, time-dependent, have jumps and the preferences of the agents are modeled by multivariate expected utility functions. We provide a complete characterization of efficient portfolios, generalizing earlier results of Dybvig (Rev Financ Stud 1:67-88, 1988) and Jouini and Kallal (J Econ Theory 66: 178-197, 1995). We basically show that a portfolio is efficient if and only if it is cyclically anticomonotonic with respect to at least one consistent price system that prices it. Finally, we introduce the notion of utility price of a given contingent claim as the minimal amount of a given initial portfolio allowing any agent to reach the claim by trading, and give a dual representation of it as the largest proportion of the market price necessary for all agents to reach the same expected utility level. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
 author = {Campi, Luciano and Jouini, Ely{\`{e}}s and Porte, Vincent},
 doi = {10.1007/s11579-013-0099-4},
 file = {:C\:/S/Library/Campi, Jouini, Porte/2013_Efficient portfolios in financial markets with proportional transaction costs.pdf:pdf},
 issn = {18629679},
 journal = {Mathematics and Financial Economics},
 keywords = {Cyclic anticomonotonicity,Duality,Efficient portfolios,Jouini,Proportional transaction costs,Utility maximization,Utility price},
 mendeley-tags = {Jouini},
 number = {3},
 pages = {281--304},
 title = {{Efficient portfolios in financial markets with proportional transaction costs}},
 volume = {7},
 year = {2013}
}

@article{Carlier2003,
 abstract = {This paper characterizes the core of a differentiable convex distortion of a probability measure on a nonatomic space by identifying it with the set of densities which dominate the derivative of the distortion, for second order stochastic dominance. The densities that have the same distribution as the derivative of the distortion are the extreme points of the core. These results are applied to the differentiability of a Yaari's or Rank Dependent Expected utility function. The superdifferential of a Choquet integral at any point is fully characterized. Examples of use of these results in simple models where some agent is a RDEU maximizer are given. {\textcopyright} 2003 Elsevier Science (USA). All rights reserved.},
 author = {Carlier, G. and Dana, R. A.},
 doi = {10.1016/S0022-0531(03)00122-4},
 file = {:C\:/S/Library/Carlier, Dana/2003_Core of convex distortions of a probability.pdf:pdf},
 isbn = {0022-0531},
 issn = {00220531},
 journal = {Journal of Economic Theory},
 keywords = {Capacity,Convex distortion,Core,Derivative and superdifferential of a Choquet inte,Risk Measures},
 mendeley-tags = {Risk Measures},
 number = {2},
 pages = {199--222},
 title = {{Core of convex distortions of a probability}},
 volume = {113},
 year = {2003}
}

@article{Carr1999,
 abstract = {2500-5000 citations 2870 citations},
 author = {Carr, Peter and Madan, Dilip},
 doi = {10.21314/jcf.1999.043},
 file = {:C\:/S/Library/Carr, Madan/1999_Option valuation using the fast Fourier transform.pdf:pdf},
 issn = {14601559},
 journal = {The Journal of Computational Finance},
 keywords = {FFT,GSC,HIGHCITE},
 mendeley-tags = {FFT,GSC,HIGHCITE},
 number = {4},
 pages = {61--73},
 title = {{Option valuation using the fast Fourier transform}},
 volume = {2},
 year = {1999}
}

@article{Cerny2004,
 author = {{\v{C}}ern{\'{y}}, Ale{\v{s}}},
 file = {:C\:/S/Library/{\v{C}}ern{\'{y}}/2004_Introduction to Fast Fourier Transform in Finance.pdf:pdf},
 isbn = {0954016157},
 journal = {Journal of Derivatives},
 keywords = {FFT,cfd,transition modelling},
 mendeley-tags = {FFT},
 number = {1},
 pages = {73--88},
 title = {{Introduction to Fast Fourier Transform in Finance}},
 volume = {12},
 year = {2004}
}

@article{Cherny2011,
 abstract = {We compare two approaches to the coherent risk contribution: the directional risk contribution is defined as where $\rho$ is a coherent risk measure; the linear risk contribution $\rho$l (X; Y) is defined through a set of axioms, one of which is the linearity in X. The linear risk contribution exists and is unique for any $\rho$ from the Weighted V@R class. We provide the representation for both risk contributions in the general setting as well as in some examples, including the MINV@R risk measure defined as where X1, . . . , XN are independent copies of X.},
 author = {Cherny, Alexander and Orlov, Dmitri},
 doi = {10.1111/j.1467-9965.2010.00441.x},
 file = {:C\:/S/Library/Cherny, Orlov/2011_On two approaches to coherent risk contribution.pdf:pdf},
 issn = {09601627},
 journal = {Mathematical Finance},
 keywords = {Cherny,Coherent risk measure,Conditional V@R,Directional risk contribution,Linear risk contribution,MINV@R,Minimal extreme measure,Weighted V@R},
 mendeley-tags = {Cherny},
 number = {3},
 pages = {557--571},
 title = {{On two approaches to coherent risk contribution}},
 volume = {21},
 year = {2011}
}

@article{Clark2004,
 abstract = {A Primer on the Exponential Family of Distributions},
 author = {Clark, David R and Thayer, Charles A},
 file = {:C\:/S/Library/Clark, Thayer/2004_A primer on the exponential family of distributions.pdf:pdf},
 journal = {Casualty Actuarial Society Spring Forum},
 pages = {117--148},
 title = {{A primer on the exponential family of distributions}},
 url = {papers2://publication/uuid/C7FF0B8F-F767-4F09-9714-F0D3711A30D8},
 year = {2004}
}

@article{Clark2014,
 author = {Clark, David R},
 file = {:C\:/S/Library/Clark/2014_Basics of Reinsurance Pricing Actuarial Study Note.pdf:pdf},
 journal = {CAS Study Note},
 title = {{Basics of Reinsurance Pricing Actuarial Study Note}},
 url = {http://www.casact.org/library/studynotes/Clark_2014.pdf},
 year = {2014}
}

@book{Conover1999,
 author = {Conover, W J},
 edition = {Third},
 publisher = {John Wiley and Sons},
 title = {{Practical nonparametric statistics}},
 year = {1999}
}

@article{Consul1973,
 abstract = {Several discrete Lagrangian probability distributions have been generated by Consul and Shenton (1972) by using the Lagrange expansion in y of a probability generating function f(x) under the transformation x y g(s) where g(x) is another pgf, By using probabilistic arguments the authors show that the transformation x y gWi oecurs naturally in the distribution of the number of customers served during a busy period which implies that at least one particular family of these Lagrangian distributions must play a basic role in queueing theory. It has also been proved that under one set of conditions all discrete Lagrangian distributions approach to the normal density function while under another set of conditions they approach the inverse Gaussian density function. {\textcopyright} 1973, Taylor & Francis Group, LLC. All rights reserved.},
 author = {Consul, P. C. and Shenton, L. R.},
 doi = {10.1080/03610927308827073},
 file = {:C\:/S/Library/Consul, Shenton/1973_Some interesting properties of Lagrangian distributions.pdf:pdf},
 issn = {00903272},
 journal = {Communications in Statistics},
 number = {3},
 pages = {263--272},
 title = {{Some interesting properties of Lagrangian distributions}},
 volume = {2},
 year = {1973}
}

@article{Culp2009,
 author = {Culp, Christopher L. and O'Donnell, Kevin J.},
 doi = {10.1108/15265940911001367},
 file = {:C\:/S/Library/Culp, O'Donnell/2009_Catastrophe reinsurance and risk capital in the wake of the credit crisis.pdf:pdf},
 issn = {1526-5943},
 journal = {The Journal of Risk Finance},
 keywords = {credit,finance,paper type conceptual paper,reinsurance,risk management,the journal of risk},
 number = {5},
 pages = {430--459},
 title = {{Catastrophe reinsurance and risk capital in the wake of the credit crisis}},
 url = {http://www.emeraldinsight.com/10.1108/15265940911001367},
 volume = {10},
 year = {2009}
}

@article{Cummins2005,
 author = {Cummins, J. David and Phillips, Richard D.},
 file = {:C\:/S/Library/Cummins, Phillips/2005_Estimating the Cost of Equity Capital for Property-Liability Insurers.pdf:pdf},
 journal = {Journal of Risk and Insurance},
 keywords = {Cummins},
 mendeley-tags = {Cummins},
 number = {3},
 pages = {441--478},
 title = {{Estimating the Cost of Equity Capital for Property-Liability Insurers}},
 url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1539-6975.2005.00132.x/full},
 volume = {72},
 year = {2005}
}

@article{Delbaen2000,
 abstract = {Sculoa notes, from his website https://people.math.ethz.ch/$\sim$delbaen/},
 author = {Delbaen, Freddy},
 doi = {10.1007/BF02809088},
 file = {:C\:/S/Library/Delbaen/2000_Coherent risk measures (Pisa Notes).pdf:pdf},
 issn = {0012-0200},
 journal = {Pisa Notes},
 keywords = {Delbaen,Risk Measures},
 mendeley-tags = {Delbaen,Risk Measures},
 number = {4},
 pages = {733--739},
 title = {{Coherent risk measures (Pisa Notes)}},
 volume = {24},
 year = {2000}
}

@article{Denault2001,
 abstract = {567 citations},
 author = {Denault, Michel},
 doi = {10.21314/jor.2001.053},
 file = {:C\:/S/Library/Denault/2001_Coherent allocation of risk capital.pdf:pdf},
 issn = {14651211},
 journal = {The Journal of Risk},
 keywords = {Risk Measures,allocation of capital,aumann-shapley,coherent risk measure,formance measure,fuzzy games,game theory,risk-adjusted per-,shapley value},
 mendeley-tags = {Risk Measures},
 number = {1},
 pages = {1--34},
 title = {{Coherent allocation of risk capital}},
 url = {ftp://ftp.sam.math.ethz.ch/pub/risklab/papers/CoherentAllocation.pdf},
 volume = {4},
 year = {2001}
}

@article{Denuit2012,
 abstract = {Using a standard reduction argument based on conditional expectations, this paper argues that risk sharing is always beneficial (with respect to convex order or second degree stochastic dominance) provided the risk-averse agents share the total losses appropriately (whatever the distribution of the losses, their correlation structure and individual degrees of risk aversion). Specifically, all agents hand their individual losses over to a pool and each of them is liable for the conditional expectation of his own loss given the total loss of the pool. We call this risk sharing mechanism the conditional mean risk sharing. If all the conditional expectations involved are non-decreasing functions of the total loss then the conditional mean risk sharing is shown to be Pareto-optimal. Explicit expressions for the individual contributions to the pool are derived in some special cases of interest: independent and identically distributed losses, comonotonic losses, and mutually exclusive losses. In particular, conditions under which this payment rule leads to a comonotonic risk sharing are examined. {\textcopyright} 2012 Elsevier B.V.},
 author = {Denuit, Michel and Dhaene, Jan},
 doi = {10.1016/j.insmatheco.2012.04.005},
 file = {:C\:/S/Library/Denuit, Dhaene/2012_Convex order and comonotonic conditional mean risk sharing.pdf:pdf},
 issn = {01676687},
 journal = {Insurance: Mathematics and Economics},
 keywords = {Comonotonicity,Conditional expectation,Pareto-optimality,Risk sharing,Stochastic orders},
 number = {2},
 pages = {265--270},
 publisher = {Elsevier B.V.},
 title = {{Convex order and comonotonic conditional mean risk sharing}},
 url = {http://dx.doi.org/10.1016/j.insmatheco.2012.04.005},
 volume = {51},
 year = {2012}
}

@article{Denuit2019,
 abstract = {Using risk-reducing properties of conditional expectations with respect to convex order, Denuit and Dhaene [Denuit, M. and Dhaene, J. (2012). Insurance: Mathematics and Economics 51, 265-270] proposed the conditional mean risk sharing rule to allocate the total risk among participants to an insurance pool. This paper relates the conditional mean risk sharing rule to the size-biased transform when pooled risks are independent. A representation formula is first derived for the conditional expectation of an individual risk given the aggregate loss. This formula is then exploited to obtain explicit expressions for the contributions to the pool when losses are modeled by compound Poisson sums, compound Negative Binomial sums, and compound Binomial sums, to which Panjer recursion applies. Simple formulas are obtained when claim severities are homogeneous. A couple of applications are considered: first, to a peer-to-peer insurance scheme where participants share the first layer of their respective risks while the higher layer is ceded to a (re)insurer; second, to survivor credits to be shared among surviving participants in tontine schemes.},
 author = {Denuit, Michel},
 doi = {10.1017/asb.2019.24},
 file = {:C\:/S/Library/Denuit/2019_Size-biased transform and conditional mean risk sharing, with application to p2p insurance and tontines.pdf:pdf},
 issn = {17831350},
 journal = {ASTIN Bulletin},
 keywords = {Conditional expectation,Panjer family of distributions,compound distributions,risk measures,risk pooling},
 number = {3},
 pages = {591--617},
 title = {{Size-biased transform and conditional mean risk sharing, with application to p2p insurance and tontines}},
 volume = {49},
 year = {2019}
}

@techreport{Denuit2020e,
 author = {Denuit, Michel M. and Robert, C Y},
 file = {:C\:/S/Library/Denuit, Robert/2020_Risk Reduction by Conditional Mean Risk Sharing With Application to Collaborative Insurance.pdf:pdf},
 institution = {UC Louvain},
 series = {Discussion Paper},
 title = {{Risk Reduction by Conditional Mean Risk Sharing With Application to Collaborative Insurance}},
 year = {2020}
}

@article{Denuit2022b,
 abstract = {Survivor funds are financial arrangements where participants agree to share the proceeds of a collective investment pool in a predescribed way depending on their survival. This offers investors a way to benefit from mortality credits, boosting financial returns. Following Denuit (2019, ASTIN Bulletin, 49, 591-617), participants are assumed to adopt the conditional mean risk sharing rule introduced in Denuit and Dhaene (2012, Insurance: Mathematics and Economics, 51, 265-270) to assess their respective shares in mortality credits. This paper looks at pools of individuals that are heterogeneous in terms of their survival probability and their contributions. Imposing mild conditions, we show that individual risk can be fully diversified if the size of the group tends to infinity. For large groups, we derive simple, hierarchical approximations of the conditional mean risk sharing rule.},
 author = {Denuit, Michel and Hieber, Peter and Robert, Christian Y.},
 doi = {10.1017/asb.2022.13},
 file = {:C\:/S/Library/Denuit, Hieber, Robert/2022_Mortality Credits Within Large Survivor Funds.pdf:pdf},
 issn = {17831350},
 journal = {ASTIN Bulletin},
 keywords = {FFT,Mortality risk pooling,conditional mean risk sharing,tontine},
 mendeley-tags = {FFT},
 number = {3},
 pages = {813--834},
 title = {{Mortality Credits Within Large Survivor Funds}},
 volume = {52},
 year = {2022}
}

@article{DeWaegenaere2003,
 abstract = {Was refd as 2006 Choquet pricing and in particular Wang's premium principle, have recently been put forward as an alternative to traditional pricing principles in finance and insurance. With Choquet pricing, the price of an insurance contract or financial asset equals the Choquet integral of the corresponding payoff with respect to a concave sub-additive measure. Since the resulting pricing rule is non-linear, existing theories do not provide an answer to the question of whether equilibrium exists. We introduce a general equilibrium model that allows for non-linearity, and show that Choquet pricing is consistent with general equilibrium. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
 author = {{De Waegenaere}, Anja and Kast, Robert and Lapied, Andre},
 doi = {10.1016/S0167-6687(03)00116-1},
 file = {:C\:/S/Library/De Waegenaere, Kast, Lapied/2003_Choquet pricing and equilibrium.pdf:pdf},
 issn = {01695002},
 journal = {Insurance: Mathematics and Economics},
 keywords = {Asset pricing,Choquet integrals,Equilibrium,Insurance pricing},
 pages = {359--370},
 title = {{Choquet pricing and equilibrium}},
 volume = {32},
 year = {2003}
}

@article{Dhaene2012b,
 author = {Dhaene, Jan and Kukush, Alexander and Linders, Daniel and Tang, Qihe},
 file = {:C\:/S/Library/Dhaene et al/2012_Remarks on quantiles and distortion risk measures.pdf:pdf},
 journal = {European Actuarial Journal},
 keywords = {comonotonicity,distorted expectation,distortion risk measure,tvar},
 number = {2},
 pages = {319--328},
 title = {{Remarks on quantiles and distortion risk measures}},
 volume = {2},
 year = {2012}
}

@article{Duan2012,
 abstract = {Any financial asset that is openly traded has a market price. Except for extreme market conditions, market price may be more or less than a “fair” value. Fair value is likely to be some complicated function of the current intrinsic value of tangible or intangible assets underlying the claim and our assessment of the characteristics of the underlying assets with respect to the expected rate of growth, future dividends, volatility, and other relevant market factors. Some of these factors that affect the price can be measured at the time of a transaction with reasonably high accuracy. Most factors, however, relate to expectations about the future and to subjective issues, such as current management, corporate policies and market environment, that could affect the future financial performance of the underlying assets. Models are thus needed to describe the stochastic factors and environment, and their implementations inevitably require computational finance tools.},
 author = {Duan, Jin Chuan and H{\"{a}}rdle, Wolfgang Karl and Gentle, James E.},
 doi = {10.1007/978-3-642-17254-0},
 file = {:C\:/S/Library/Duan, H{\"{a}}rdle, Gentle/2012_Handbook of computational finance.pdf:pdf},
 isbn = {9783642172540},
 journal = {Handbook of Computational Finance},
 keywords = {FFT},
 mendeley-tags = {FFT},
 pages = {1--804},
 title = {{Handbook of computational finance}},
 year = {2012}
}

@article{Embrechts1993,
 abstract = {Transform methods, together with the fast Fourier transform algorithm, can be used to compute various quantities of interest in risk theory and insurance mathematics. These include the total claim size distribution at a fixed time, the mean and variance of the claim size process as a function of time in the Sparre‐Andersen model, and the probability of ruin. The associated discretization error can be reduced by applying Richardson's deferred approach to the limit. A theorem is given that puts the use of this technique on a mathematical basis in the context of compound distributions. Copyright {\textcopyright} 1993, Wiley Blackwell. All rights reserved},
 author = {Embrechts, P. and Gr{\"{u}}bel, R. and Pitts, S. M.},
 doi = {10.1111/j.1467-9574.1993.tb01406.x},
 file = {:C\:/S/Library/Embrechts, Gr{\"{u}}bel, Pitts/1993_Some applications of the fast Fourier transform algorithm in insurance mathematics.pdf:pdf},
 issn = {14679574},
 journal = {Statistica Neerlandica},
 keywords = {FFT,Richardson extrapolation,Sparre‐Anderson model,compound distributions,fast Fourier transform},
 mendeley-tags = {FFT},
 number = {1},
 pages = {59--75},
 title = {{Some applications of the fast Fourier transform algorithm in insurance mathematics}},
 volume = {47},
 year = {1993}
}

@book{Embrechts1997,
 abstract = {5K-10K Citations},
 address = {Berlin Heidelberg},
 author = {Embrechts, Paul and Kl{\"{u}}ppelberg, Claudia and Mikosch, Thomas},
 doi = {10.1007/978-3-642-33483-2},
 file = {:C\:/S/Library/Embrechts, Kl{\"{u}}ppelberg, Mikosch/1997_Modelling Extremal Events.pdf:pdf},
 keywords = {Book,GSC},
 mendeley-tags = {Book,GSC},
 publisher = {Springer Verlag},
 title = {{Modelling Extremal Events}},
 year = {1997}
}

@article{Embrechts2009a,
 abstract = {Numerical evaluation of compound distributions is an important task in insurance mathematics and quantitative risk management. In practice, both recursive methods as well as transform based techniques are widely used. We give a survey of these tools, point out the respective merits and provide some numerical examples. {\textcopyright} 2008 Springer-Verlag.},
 author = {Embrechts, Paul and Frei, Marco},
 doi = {10.1007/s00186-008-0249-2},
 file = {:C\:/S/Library/Embrechts, Frei/2009_Panjer recursion versus FFT for compound distributions.pdf:pdf},
 issn = {14322994},
 journal = {Mathematical Methods of Operations Research},
 keywords = {Compound distributions,FFT,Fast Fourier transform,Panjer recursion,Risk management},
 mendeley-tags = {FFT},
 number = {3},
 pages = {497--508},
 title = {{Panjer recursion versus FFT for compound distributions}},
 volume = {69},
 year = {2009}
}

@article{Embrechts2013,
 abstract = {Despite well-known shortcomings as a risk measure, Value-at-Risk (VaR) is still the industry and regulatory standard for the calculation of risk capital in banking and insurance. This paper is concerned with the numerical estimation of the VaR for a portfolio position as a function of different dependence scenarios on the factors of the portfolio. Besides summarizing the most relevant analytical bounds, including a discussion of their sharpness, we introduce a numerical algorithm which allows for the computation of reliable (sharp) bounds for the VaR of high-dimensional portfolios with dimensions d possibly in the several hundreds. We show that additional positive dependence information will typically not improve the upper bound substantially. In contrast higher order marginal information on the model, when available, may lead to strongly improved bounds. Several examples of practical relevance show how explicit VaR bounds can be obtained. These bounds can be interpreted as a measure of model uncertainty induced by possible dependence scenarios. ?? 2013 Elsevier B.V.},
 author = {Embrechts, Paul and Puccetti, Giovanni and Ruschendorf, Ludger},
 doi = {10.1016/j.jbankfin.2013.03.014},
 file = {:C\:/S/Library/Embrechts, Puccetti, Ruschendorf/2013_Model uncertainty and VaR aggregation.pdf:pdf},
 isbn = {3905543745},
 issn = {03784266},
 journal = {Journal of Banking and Finance},
 keywords = {Copula,Fr??chet class,Model uncertainity,Operational Risk,Positive dependence,Rearrangement algorithm,Risk Measures,Risk aggregation,Tails},
 mendeley-tags = {Risk Measures,Tails},
 number = {8},
 pages = {2750--2764},
 title = {{Model uncertainty and VaR aggregation}},
 volume = {37},
 year = {2013}
}

@book{feller71,
 abstract = {The exponential and the uniform densities; Special densities. Randomization; Densities in higher dimensions. Normal densities and processes; Probability measures and spaces; Probability distributions in Rr; A survey of some important distributions and processes; Laws of large numbers. Aplications in analysis; The basic limit theorems; Infinitely divisible distributions and semi-groups; Markov processes and semi-groups; Renewal theory; Random walks in R1; Laplace transforms. Tauberian theorems. Resolvents; Aplications of Laplace transforms; Characteristic functions; Expansions related to the central limit theorem; Infinitely divisible distributions; Applications of Fourier methods to ramdom walks; harmonic analysis; Answers to problems.},
 author = {Feller, William},
 edition = {Second},
 isbn = {0471257095},
 pages = {669},
 publisher = {J. Wiley and Sons},
 title = {{An Introduction to Probability Theory and its Applications, Volume 2}},
 year = {1971}
}

@book{Follmer2011,
 author = {F{\"{o}}llmer, Hans and Schied, Alexander},
 edition = {Third Edit},
 publisher = {Walter de Gruyter},
 title = {{Stochastic finance: an introduction in discrete time}},
 year = {2011}
}

@book{Follmer2016,
 address = {Berlin, Boston},
 archiveprefix = {arXiv},
 arxivid = {arXiv:1011.1669v3},
 author = {F{\"{o}}llmer, Hans and Schied, Alexander},
 doi = {10.1017/CBO9781107415324.004},
 edition = {Fourth},
 eprint = {arXiv:1011.1669v3},
 file = {:C\:/S/Library/F{\"{o}}llmer, Schied/2016_Stochastic Finance An Introduction in Discrete Time.pdf:pdf},
 isbn = {9788578110796},
 issn = {00157120},
 keywords = {Book},
 mendeley-tags = {Book},
 pmid = {25246403},
 publisher = {Walter de Gruyter},
 title = {{Stochastic Finance: An Introduction in Discrete Time}},
 year = {2016}
}

@article{Gerber1982,
 abstract = {The paper develops a method for the numerical evaluation of the distribution of aggregate claims and its stop-loss premiums. {\textcopyright} 1982.},
 author = {Gerber, Hans U.},
 doi = {10.1016/0167-6687(82)90016-6},
 file = {:C\:/S/Library/Gerber/1982_On the numerical evaluation of the distribution of aggregate claims and its stop-loss premiums.pdf:pdf},
 issn = {01676687},
 journal = {Insurance Mathematics and Economics},
 keywords = {Compound Poisson distribution,Distribution of aggregate claims,FFT,Ordering of distribution functions,Stop-loss premiums},
 mendeley-tags = {FFT},
 number = {1},
 pages = {13--18},
 title = {{On the numerical evaluation of the distribution of aggregate claims and its stop-loss premiums}},
 volume = {1},
 year = {1982}
}

@article{Grubel1999,
 author = {Gr{\"{u}}bel, Rudolf and Hermesmeier, Renate},
 doi = {10.2143/AST.29.2.504611},
 file = {:C\:/S/Library/Gr{\"{u}}bel, Hermesmeier/1999_Computation of Compound Distributions I Aliasing Errors and Exponential Tilting.pdf:pdf},
 issn = {17831350},
 journal = {Astin Bulletin},
 keywords = {FFT,aliasing,and phrases,change of measure,fourier,random sums,ruin probabilities,total claim size distribution,transformation},
 mendeley-tags = {FFT},
 number = {2},
 pages = {197--214},
 title = {{Computation of Compound Distributions I: Aliasing Errors and Exponential Tilting}},
 volume = {29},
 year = {1999}
}

@article{Grubel2000,
 author = {Gr{\"{u}}bel, Rudolf and Hermesmeier, Renate},
 doi = {10.2143/AST.30.2.504638},
 file = {:C\:/S/Library/Gr{\"{u}}bel, Hermesmeier/2000_Computation of Compound Distributions II Discretization Errors and Richardson Extrapolation.pdf:pdf},
 issn = {17831350},
 journal = {ASTIN Bulletin},
 keywords = {FFT,acceleration of,and phrases,discretization,ruin probabilities,total claim size distribution},
 mendeley-tags = {FFT},
 number = {2},
 pages = {309--332},
 title = {{Computation of Compound Distributions II: Discretization Errors and Richardson Extrapolation}},
 volume = {30},
 year = {2000}
}

@article{Grundl2007,
 author = {Grundl, Helmut and Schmeiser, Hato},
 file = {:C\:/S/Library/Grundl, Schmeiser/2007_Capital allocation for insurance companies---What Good Is It.pdf:pdf},
 journal = {Journal of Risk and Insurance},
 keywords = {Risk Measures},
 mendeley-tags = {Risk Measures},
 number = {2},
 title = {{Capital allocation for insurance companies---What Good Is It?}},
 url = {http://www.jstor.org/stable/2691539},
 volume = {74},
 year = {2007}
}

@article{Heckman1983,
 abstract = {This paper discusses aggregate loss distributions from the perspective of collective risk theory. An accurate, efficient and practical algorithm is given for calculating cumulative probabilities and excess pure premiums. The input re-quired is the claim severity and claim count distributions. One of the main drawbacks of the collective risk model is the uncertainty of the parameters of the claim severity and claim count distributions. Modifi-cations of the collective risk model are proposed to deal with these problems. These modifications are incorporated into the algorithm. Examples are given illustrating the use of this algorithm. They include (1) calculating the pure premium for a policy with an aggregate limit; (2) calculating the pure premium of an aggregate stop-loss policy for group life insurance; and (3) calculating the insurance charge for a multi-line retrospective rating plan, including a line which is itself subject to an aggregate limit.},
 author = {Heckman, Philip E and Meyers, Glenn G},
 file = {:C\:/S/Library/Heckman, Meyers/1983_The calculation of aggregate loss distributions from claim severity and claim count distributions.pdf:pdf},
 journal = {Proceedings of the Casualty Actuarial Society},
 keywords = {Meyers},
 mendeley-tags = {Meyers},
 pages = {49--66},
 title = {{The calculation of aggregate loss distributions from claim severity and claim count distributions}},
 year = {1983}
}

@article{Homer2003,
 author = {Homer, David L and Clark, David R},
 file = {:C\:/S/Library/Homer, Clark/2003_Insurance Applications of Bivariate Distributions.pdf:pdf},
 journal = {Proceedings of the Casualty Actuarial Society},
 number = {iid},
 pages = {274--307},
 title = {{Insurance Applications of Bivariate Distributions}},
 url = {http://www.casact.org/pubs/proceed/proceed03/03274.pdf},
 volume = {90},
 year = {2003}
}

@article{Hurlimann1986,
 abstract = {Recently J. Bertram (1981) has suggested to calculate the distributions of aggregate claims in case of arbitrary risk sums by means of the numerical inversion of the Fast Fourier Transform. Since there remains an approximation error in this procedure, it is justified to ask in which measure this error propagates on stop-loss premiums. The implementation on a desk computer of our bounds for the Compound Poisson distribution have shown the very good fit of the FFT method for the insurance practice. {\textcopyright} 1986, Taylor & Francis Group, LLC. All rights reserved.},
 author = {H{\"{u}}rlimann, W.},
 doi = {10.1080/03461238.1986.10413798},
 file = {:C\:/S/Library/H{\"{u}}rlimann/1986_Error Bounds for Stop-loss Premiums Calculated with the Fast Fourier Transform.pdf:pdf},
 issn = {16512030},
 journal = {Scandinavian Actuarial Journal},
 keywords = {FFT},
 mendeley-tags = {FFT},
 number = {2},
 pages = {107--113},
 title = {{Error Bounds for Stop-loss Premiums Calculated with the Fast Fourier Transform}},
 volume = {1986},
 year = {1986}
}

@article{Hyndman1996,
 abstract = {There are a large number of different definitions used for sample quantiles in statistical computer packages. Often within the same package one definition will be used to compute a quantile explicitly, while other definitions may be used when producing a boxplot, a probability plot, or a QQ plot. We compare the most commonly implemented sample quantile definitions by writing them in a common notation and investigating their motivation and some of their properties. We argue that there is a need to adopt a standard definition for sample quantiles so that the same answers are produced by different packages and within each package. We conclude by recommending that the median-unbiased estimator be used because it has most of the desirable properties of a quantile estimator and can be defined independently of the underlying distribution. {\textcopyright} 1996 Taylor & Francis Group, LLC.},
 author = {Hyndman, Rob J. and Fan, Yanan},
 doi = {10.1080/00031305.1996.10473566},
 file = {:C\:/S/Library/Hyndman, Fan/1996_Sample Quantiles in Statistical Packages.pdf:pdf},
 issn = {15372731},
 journal = {American Statistician},
 keywords = {Percentiles,Quartiles,Sample quantiles,Statistical computer packages},
 number = {4},
 pages = {361--365},
 title = {{Sample Quantiles in Statistical Packages}},
 volume = {50},
 year = {1996}
}

@article{Ibragimov2010,
 author = {Ibragimov, Rustam and Jaffee, Dwight and Walden, Johan},
 doi = {10.1111/j.1539-6975.2010.01353.x},
 file = {:C\:/S/Library/Ibragimov, Jaffee, Walden/2010_Pricing and Capital Allocation for Multiline Insurance Firms.pdf:pdf},
 issn = {00224367},
 journal = {Journal of Risk and Insurance},
 keywords = {Risk Measures},
 mendeley-tags = {Risk Measures},
 month = {mar},
 number = {3},
 pages = {551--578},
 title = {{Pricing and Capital Allocation for Multiline Insurance Firms}},
 url = {http://doi.wiley.com/10.1111/j.1539-6975.2010.01353.x},
 volume = {77},
 year = {2010}
}

@article{Jewson2022,
 abstract = {There is great interest in trying to understand how to take climate model projections of possible changes in hurricane behaviour due to climate change and apply them to hurricane risk models. In Knutson et al. (Bull Am Meteorol Soc 101:E303–E322, 2020), projections from many climate models were combined to form distributions of possible changes in hurricane frequency and intensity. It has been shown that propagating the uncertainty represented by these distributions is necessary to estimate the impact on risk correctly. Building on these results, we now consider how distributions of changes in hurricane frequency and intensity can be applied to hurricane risk models that are formulated in the standard event loss table and year loss table formats. Because of the uncertainty, this requires the use of novel simulation and weighting techniques that extend standard methods for adjusting risk models. We demonstrate that these novel techniques work in a simple hurricane risk model. We also present new analytical solutions that show how means and variances of risk change due to the application of uncertain adjustments. Finally, we use emulators to explore how the output from just a single evaluation of a hurricane risk model can be used to derive sensitivity estimates that would otherwise require a large number of evaluations of the model. The methods we present could readily be applied to full complexity hurricane risk models and will hopefully contribute to efforts to quantify the possible effects of climate change on present and future hurricane risk.},
 author = {Jewson, Stephen},
 doi = {10.1007/s00477-022-02198-y},
 file = {:C\:/S/Library/Jewson/2022_Application of uncertain hurricane climate change projections to catastrophe risk models.pdf:pdf},
 isbn = {0123456789},
 issn = {14363259},
 journal = {Stochastic Environmental Research and Risk Assessment},
 keywords = {Catastrophe modelling,Climate change,Climate risk,Hurricanes,Tropical cyclone,Uncertainty},
 publisher = {Springer Berlin Heidelberg},
 title = {{Application of uncertain hurricane climate change projections to catastrophe risk models}},
 url = {https://doi.org/10.1007/s00477-022-02198-y},
 volume = {0123456789},
 year = {2022}
}

@article{Jewson2022b,
 author = {Jewson, Stephen},
 file = {:C\:/S/Library/Jewson/2022_Projections of Changes in U.S. Hurricane Damage Due to Projected Changes in Hurricane Frequencies(2).pdf:pdf},
 journal = {submitted},
 title = {{Projections of Changes in U.S. Hurricane Damage Due to Projected Changes in Hurricane Frequencies}},
 year = {2022}
}

@book{Jorgensen1997,
 author = {J{\o}rgensen, Bent},
 file = {:C\:/S/Library/J{\o}rgensen/1997_The theory of dispersion models(2).pdf:pdf;:C\:/S/Library/J{\o}rgensen/1997_The theory of dispersion models.pdf:pdf},
 publisher = {CRC Press},
 title = {{The theory of dispersion models}},
 year = {1997}
}

@article{Jouini2001,
 abstract = {We provide a price characterization of efficient contingent claims - that is, chosen by at least a rational agent - in multiperiod economies with market frictions. Frictions include market incompleteness, transaction costs, short-selling, and borrowing costs. We characterize the inefficiency cost of a trading strategy - its required investment minus the largest amount necessary to obtain the same utility level - and we propose a measure of portfolio performance. We show that arbitrage bounds cannot be tightened based on efficiency without restricting preferences or endowments. We observe common investment strategies becoming inefficient with market frictions and others rationalized by them.},
 author = {Jouini, Ely{\`{e}}s and Kallal, H{\'{e}}di},
 doi = {10.1093/rfs/14.2.343},
 file = {:C\:/S/Library/Jouini, Kallal/2001_Efficient trading strategies in the presence of market frictions.pdf:pdf},
 issn = {08939454},
 journal = {Review of Financial Studies},
 keywords = {Jouini},
 mendeley-tags = {Jouini},
 number = {2},
 pages = {343--369},
 title = {{Efficient trading strategies in the presence of market frictions}},
 volume = {14},
 year = {2001}
}

@book{Kaas2008,
 abstract = {This book gives a comprehensive survey of non-life insurance mathematics. Origi- nally written for use with the actuarial science programs at the Universities of Am- sterdam and Leuven, it is now in use at many other universities, as well as for the non-academic actuarial education program organized by the Dutch Actuarial So- ciety. It provides a link to the further theoretical study of actuarial science. The methods presented can not only be used in non-life insurance, but are also effective in other branches of actuarial science, as well as, of course, in actuarial practice. Apart from the standard theory, this text contains methods directly relevant for actuarial practice, for example the rating of automobile insurance policies, premium principles and riskmeasures, and IBNR models.Also, the important actuarial statis- tical tool of the Generalized Linear Models is studied. These models provide extra possibilities beyond ordinary linear models and regression, the statistical tools of choice for econometricians. Furthermore, a short introduction is given to credibil- ity theory. Another topic that always has enjoyed the attention of risk theoreticians is the study of ordering of risks. The book reflects the state of the art in actuarial risk theory; many results presented were published in the actuarial literature only recently. In},
 archiveprefix = {arXiv},
 arxivid = {arXiv:1011.1669v3},
 author = {Kaas, Rob and Goovaerts, Marc and Dhaene, Jan and Denuit, Michel},
 doi = {10.1007/978-3-540-70998-5},
 eprint = {arXiv:1011.1669v3},
 file = {:C\:/S/Library/Kaas et al/2008_Modern Actuarial Risk Theory.pdf:pdf},
 isbn = {978-3-540-70992-3},
 issn = {1098-6596},
 keywords = {Goovaerts},
 mendeley-tags = {Goovaerts},
 pmid = {25246403},
 publisher = {Springer},
 title = {{Modern Actuarial Risk Theory}},
 url = {http://link.springer.com/10.1007/978-3-540-70998-5 https://fac.ksu.edu.sa/sites/default/files/modern_acturial_risk_theory.pdf},
 year = {2008}
}

@article{Kusuoka2001,
 abstract = {500-1000 citations},
 author = {Kusuoka, Shigeo},
 file = {:C\:/S/Library/Kusuoka/2001_On law invariant coherent risk measures.pdf:pdf},
 journal = {Advances in Mathematical Economics},
 keywords = {GSC,Risk Measures},
 mendeley-tags = {GSC,Risk Measures},
 pages = {83--95},
 title = {{On law invariant coherent risk measures}},
 url = {http://link.springer.com/chapter/10.1007/978-4-431-67891-5_4},
 volume = {3},
 year = {2001}
}

@book{Loeve1955,
 author = {Loeve, Michel},
 doi = {10.1137/1006078},
 file = {:C\:/S/Library/Loeve/1955_Probability Theory.pdf:pdf},
 isbn = {9780486814889},
 issn = {0036-1445},
 keywords = {Book},
 mendeley-tags = {Book},
 publisher = {D. Van Nostrand Company},
 title = {{Probability Theory}},
 year = {1955}
}

@article{Ludwig1991,
 author = {Ludwig, B Y Stephen J},
 file = {:C\:/S/Library/Ludwig/1991_AN EXPOSURE RATING APPROACH TO PRICING(2).pdf:pdf},
 journal = {Proceedings of the Casualty Actuarial Society},
 keywords = {FFT},
 mendeley-tags = {FFT},
 title = {{AN EXPOSURE RATING APPROACH TO PRICING}},
 year = {1991}
}

@article{Luo2009,
 author = {Luo, Xiaolin and Shevchenko, Pavel V},
 file = {:C\:/S/Library/Luo, Shevchenko/2009_Computing Tails of Compound Distributions Using Direct Numerical Integration.pdf:pdf},
 journal = {Journal of Computational Finance},
 keywords = {FFT,characteristic function,compound distribution,fft,monte carlo,truncation error},
 mendeley-tags = {FFT},
 number = {2},
 pages = {1--33},
 title = {{Computing Tails of Compound Distributions Using Direct Numerical Integration}},
 volume = {13},
 year = {2009}
}

@article{Luo2010,
 archiveprefix = {arXiv},
 arxivid = {arXiv:1005.1705v1},
 author = {Luo, Xiaolin and Shevchenko, Pavel V},
 eprint = {arXiv:1005.1705v1},
 file = {:C\:/S/Library/Luo, Shevchenko/2011_A Short Tale of Long Tail Integration.pdf:pdf},
 journal = {Numerical Algorithms},
 keywords = {FFT,fourier transform,laplace transform,numerical integration,trunca-},
 mendeley-tags = {FFT},
 number = {4},
 pages = {577--590},
 title = {{A Short Tale of Long Tail Integration}},
 volume = {56},
 year = {2011}
}

@article{Major2018,
 abstract = {This paper extends the evaluation and allocation of distortion risk measures to apply to arbitrary homogeneous components of a portfolio (“financial derivatives,” e.g. reinsurance recovery, of primitive portfolio components, e.g. lines of business). It is argued that the allocation of the portfolio measure to the financial derivative takes the usual form of (distortion-) weighted “co-measure” expectation. Due to homogeneity, the allocation of the derivative's value to further subcomponents (ultimately, the primitive elements of the portfolio), following Aumann-Shapley, is the exposure gradient. However, the gradient in this case consists of two terms. The first is the familiar distorted expectation of the gradient of the component with respect to the subcomponent. The second term involves the conditional covariance of the component with the subcomponent. Sufficient conditions for this second term to vanish are provided. A method for estimating the second component in a simulation framework is proposed.},
 author = {Major, John A.},
 doi = {10.2139/ssrn.2972955},
 file = {:C\:/S/Library/Major/2018_Distortion Measures on Homogeneous Financial Derivatives.pdf:pdf},
 issn = {1556-5068},
 journal = {Insurance: Mathematics and Economics},
 keywords = {Major,Risk Measures,aumann-shapley,c71,capital allocation,d81,distortion measures,financial derivatives,g22,jel classifications,reinsurance},
 mendeley-tags = {Major,Risk Measures},
 pages = {82--91},
 publisher = {Elsevier B.V.},
 title = {{Distortion Measures on Homogeneous Financial Derivatives}},
 url = {http://www.ssrn.com/abstract=2972955 http://linkinghub.elsevier.com/retrieve/pii/S0167668717303384},
 volume = {79},
 year = {2018}
}

@article{Major2020,
 abstract = {We analyze multiline pricing and capital allocation in equilibrium no-arbitrage markets. Existing theories often assume a perfect complete market, but when pricing is linear, there is no diversification benefit from risk pooling and therefore no role for insurance companies. Instead of a perfect market, we assume a non-additive distortion pricing functional and the principle of equal priority of payments in default. Under these assumptions, we derive a canonical allocation of premium and margin, with properties that merit the name the natural allocation. The natural allocation gives non-negative margins to all independent lines for default-free insurance but can exhibit negative margins for low-risk lines under limited liability. We introduce novel conditional expectation measures of relative risk within a portfolio and use them to derive simple, intuitively appealing expressions for risk margins and capital allocations. We give a unique capital allocation consistent with our law invariant pricing functional. Such allocations produce returns that vary by line, in contrast to many other approaches. Our model provides a bridge between the theoretical perspective that there should be no compensation for bearing diversifiable risk and the empirical observation that more risky lines fetch higher margins relative to subjective expected values.},
 archiveprefix = {arXiv},
 arxivid = {2008.12427},
 author = {Major, John A. and Mildenhall, Stephen J.},
 eprint = {2008.12427},
 file = {:C\:/S/Library/Major, Mildenhall/2020_Pricing and Capital Allocation for Multiline Insurance Firms With Finite Assets in an Imperfect Market.pdf:pdf},
 journal = {Arxiv},
 keywords = {Major},
 mendeley-tags = {Major},
 number = {2008.12427},
 pages = {1--33},
 title = {{Pricing and Capital Allocation for Multiline Insurance Firms With Finite Assets in an Imperfect Market}},
 url = {http://arxiv.org/abs/2008.12427},
 year = {2020}
}

@article{Mango2005a,
 author = {Mango, Donald},
 file = {:C\:/S/Library/Mango/2005_Insurance Capital as a Shared Asset.pdf:pdf},
 journal = {Astin Bulletin},
 keywords = {Risk Measures},
 mendeley-tags = {Risk Measures},
 number = {2},
 pages = {471--486},
 title = {{Insurance Capital as a Shared Asset}},
 url = {https://www.beanactuary.com/pubs/forum/06fforum/577.pdf},
 volume = {35},
 year = {2005}
}

@article{Mango2013,
 author = {Mango, Donald and Major, John and Adler, Avraham and Bunick, Claude},
 file = {:C\:/S/Library/Mango et al/2013_Capital Tranching A RAROC Approach to Assessing Reinsurance Cost Effectiveness.pdf:pdf},
 journal = {Variance},
 keywords = {Major,capital consumption,raroc,reinsurance cost effectiveness,risk management,rorac},
 mendeley-tags = {Major},
 number = {September},
 pages = {82--91},
 title = {{Capital Tranching: A RAROC Approach to Assessing Reinsurance Cost Effectiveness}},
 url = {http://www.actuaries.org.uk/sites/all/files/documents/pdf/capital-tranching-raroc-approach-assessing-reinsurance-cost-effectiveness.pdf},
 volume = {7},
 year = {2013}
}

@article{Marinacci2004b,
 abstract = {We establish a calculus characterization of the core of supermodular games, which reduces the description of the core to the computation of suitable Gateaux derivatives of the Choquet integrals associated with the game. Our result generalizes a classic result of Shapley (Internat. J. Game Theory 1 (1971) 11) to infinite games. As an application, we show that this representation takes a stark form for supermodular measure games. {\textcopyright} 2003 Elsevier Inc. All rights reserved.},
 author = {Marinacci, Massimo and Montrucchio, Luigi},
 doi = {10.1016/S0022-0531(03)00258-8},
 file = {:C\:/S/Library/Marinacci, Montrucchio/2004_A characterization of the core of convex games through Gateaux derivatives.pdf:pdf},
 issn = {00220531},
 journal = {Journal of Economic Theory},
 keywords = {Risk Measures},
 mendeley-tags = {Risk Measures},
 number = {2},
 pages = {229--248},
 publisher = {Elsevier Inc.},
 title = {{A characterization of the core of convex games through Gateaux derivatives}},
 url = {http://dx.doi.org/10.1016/S0022-0531(03)00258-8},
 volume = {116},
 year = {2004}
}

@inproceedings{Mata2002,
 author = {Mata, Ana J and Ph, D and Fannin, Brian and Verheyen, Mark A},
 booktitle = {General Insurance Convention},
 file = {:C\:/S/Library/Mata et al/2002_Pricing Excess of Loss Treaty with Loss Sensitive Features An Exposure Rating Approach.pdf:pdf},
 title = {{Pricing Excess of Loss Treaty with Loss Sensitive Features: An Exposure Rating Approach}},
 year = {2002}
}

@article{McGuinness1969,
 author = {McGuinness, John S},
 file = {:C\:/S/Library/McGuinness/1969_Is “probable maximum loss” (PML) a useful concept.pdf:pdf},
 journal = {Proceedings of Casualty Actuarial Society},
 number = {May},
 pages = {31--48},
 title = {{Is “probable maximum loss” (PML) a useful concept?}},
 volume = {LVI},
 year = {1969}
}

@article{Menn2006,
 abstract = {An algorithm for the approximation of $\alpha$-stable densities is developed and compared with similar approximation methodologies. The proposed approach employs an adaptive Simpson rule for the quadrature of the Fourier inversion integral and asymptotic Bergstr{\"{o}}m series expansions for the tails of the density. It is guaranteed that the approximation integrates precisely to unity which is helpful for numerical maximum-likelihood routines. The accuracy of the algorithm has been verified with respect to the values obtained by Nolan's program STABLE for a grid of parameter values. It is shown that a significant reduction of the computational effort with respect to Nolan's program can be achieved while maintaining a satisfying accuracy. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
 author = {Menn, Christian and Rachev, Svetlozar T},
 doi = {10.1016/j.csda.2005.03.004},
 file = {:C\:/S/Library/Menn, Rachev/2006_Calibrated FFT-based density approximations for $\alpha$-stable distributions.pdf:pdf},
 issn = {01679473},
 journal = {Computational Statistics and Data Analysis},
 keywords = {Bergstr{\"{o}}m expansion,Density approximation,FFT,Fast Fourier transformation,Stable distribution},
 mendeley-tags = {FFT},
 number = {8},
 pages = {1891--1904},
 title = {{Calibrated FFT-based density approximations for $\alpha$-stable distributions}},
 volume = {50},
 year = {2006}
}

@article{Meyers1996,
 author = {Meyers, Glenn G},
 file = {:C\:/S/Library/Meyers/1996_The competitive market equilibrium risk load formula for catastrophe ratemaking.pdf:pdf},
 journal = {PCAS},
 keywords = {Meyers},
 mendeley-tags = {Meyers},
 pages = {563--600},
 title = {{The competitive market equilibrium risk load formula for catastrophe ratemaking}},
 year = {1996}
}

@article{Meyers2019,
 author = {Meyers, Glenn G},
 file = {:C\:/S/Library/Meyers/2019_A Cost of Capital Risk Margin Formula For Non-Life Insurance Liabilities.pdf:pdf},
 journal = {Variance},
 keywords = {Meyers,bayesian mcmc,capital requirements,risk margins,stochastic loss reserving},
 mendeley-tags = {Meyers},
 number = {2},
 pages = {186--198},
 title = {{A Cost of Capital Risk Margin Formula For Non-Life Insurance Liabilities}},
 volume = {12},
 year = {2019}
}

@article{Mildenhall2004,
 author = {Mildenhall, Stephen J},
 file = {:C\:/S/Library/Mildenhall/2004_A Note on the Myers and Read Capital Allocation Formula.pdf:pdf},
 issn = {10920277},
 journal = {North American Actuarial Journal},
 keywords = {capital and ownership structure,financing policy,g - financial economics,g220 - insurance,g320 -,insurance companies},
 number = {2},
 pages = {32--44},
 title = {{A Note on the Myers and Read Capital Allocation Formula}},
 url = {http://library.soa.org/library-pdf/naaj0402_3.pdf},
 volume = {8},
 year = {2004}
}

@article{Mildenhall2005a,
 author = {Mildenhall, Stephen J},
 file = {:C\:/S/Library/Mildenhall/2005_Correlation and Aggregate Loss Distributions With An Emphasis On The Iman-Conover Method.pdf:pdf},
 journal = {Casualty Actuarial Society Forum},
 keywords = {FFT},
 mendeley-tags = {FFT},
 title = {{Correlation and Aggregate Loss Distributions With An Emphasis On The Iman-Conover Method}},
 volume = {Winter},
 year = {2005}
}

@article{Mildenhall2017b,
 annote = {Quick summary for Risks cover. 

Actuarial Geometry studies how the shape of an aggregate loss distribution changes as expected loss volume changes. The theory of Markov processes implies Levy processes are straight lines even though their distribution changes shape as expected losses increase. In contrast, an asset-return model retains a constant shape but represents a curved path. The difference is significant in the theory of risk measures and capital allocation, which are based on marginal changes in loss volume. In the figure the Levy process (red) is a great circle straight line whereas the asset model (blue) is a curved path. Growth along the two paths results in different measures of marginal risk (top right).},
 author = {Mildenhall, Stephen J},
 doi = {10.3390/risks5020031},
 file = {:C\:/S/Library/Mildenhall/2017_Actuarial Geometry.pdf:pdf},
 issn = {2227-9091},
 journal = {Risks},
 keywords = {capital allocation,capital determination,game,risk measure},
 number = {31},
 title = {{Actuarial Geometry}},
 volume = {5},
 year = {2017}
}

@article{Mildenhall2022,
 author = {Mildenhall, Stephen J.},
 doi = {10.1016/j.insmatheco.2022.04.006},
 file = {:C\:/S/Library/Mildenhall/2022_Similar Risks Have Similar Prices A Useful and Exact Quantification.pdf:pdf},
 issn = {01676687},
 journal = {Insurance: Mathematics and Economics},
 pages = {203--210},
 publisher = {Elsevier B.V.},
 title = {{Similar Risks Have Similar Prices: A Useful and Exact Quantification}},
 url = {https://doi.org/10.1016/j.insmatheco.2022.04.006},
 volume = {105},
 year = {2022}
}

@book{Mildenhall2022a,
 author = {Mildenhall, Stephen J. and Major, John A.},
 file = {:C\:/S/Library/Mildenhall, Major/2022_Pricing Insurance Risk Theory and Practice.pdf:pdf},
 isbn = {9781119130536},
 publisher = {John Wiley & Sons, Inc.},
 title = {{Pricing Insurance Risk: Theory and Practice}},
 year = {2022}
}

@article{Milevsky1998,
 author = {Milevsky, Moshe Arye and Posner, Steven E.},
 file = {:C\:/S/Library/Milevsky, Posner/1998_Asian Options, the Sum of Lognormals , and the Reciprocal Gamma Distribution.pdf:pdf},
 journal = {Journal of Financial and Quantitative Analysis},
 number = {3},
 pages = {409--422},
 title = {{Asian Options, the Sum of Lognormals , and the Reciprocal Gamma Distribution}},
 volume = {33},
 year = {1998}
}

@book{MitchellWallace2017,
 author = {Mitchell-Wallace, Kirsten and Jones, Matthew and Hillier, John and Foote, Matthew},
 chapter = {Chapter 1},
 file = {:C\:/S/Library/Mitchell-Wallace et al/2017_Natural Catastrophe Risk Managment and Modeling - A Practitioner's Guide.pdf:pdf},
 publisher = {Wiley-Blackwell},
 title = {{Natural Catastrophe Risk Managment and Modeling - A Practitioner's Guide}},
 year = {2017}
}

@incollection{Myers1987,
 author = {Myers, Stewart C and Cohn, Richard A},
 booktitle = {Fair Rate of Return in Property-Liability Insurance},
 pages = {55--78},
 publisher = {Springer},
 title = {{A discounted cash flow approach to property-liability insurance rate regulation}},
 year = {1987}
}

@article{Myers2001,
 abstract = {250-500 citations},
 author = {Myers, Stewart C and {Read Jr.}, James A},
 file = {:C\:/S/Library/Myers, Read Jr/2001_Capital allocation for insurance companies.pdf:pdf},
 journal = {Journal of Risk and Insurance},
 keywords = {GSC,Risk Measures},
 mendeley-tags = {GSC,Risk Measures},
 number = {4},
 pages = {545--580},
 title = {{Capital allocation for insurance companies}},
 url = {http://www.jstor.org/stable/2691539},
 volume = {68},
 year = {2001}
}

@article{Panjer1983,
 author = {Panjer, Harry H. and Lutek, B. W.},
 file = {:C\:/S/Library/Panjer, Lutek/1983_Practical aspects of stop-loss calculations.pdf:pdf},
 journal = {Insurance: Mathematics and Economics},
 keywords = {FFT},
 mendeley-tags = {FFT},
 pages = {159--177},
 title = {{Practical aspects of stop-loss calculations}},
 volume = {2},
 year = {1983}
}

@article{Papush2001,
 author = {Papush, Dmitry E and Patrik, Gary S and Podgaits, Felix},
 file = {:C\:/S/Library/Papush, Patrik, Podgaits/2001_Approximations of the Aggregate Loss Distribution.pdf:pdf},
 journal = {Casualty Actuarial Society Forum},
 keywords = {FFT},
 mendeley-tags = {FFT},
 pages = {175--186},
 title = {{Approximations of the Aggregate Loss Distribution}},
 url = {http://www.casact.org/pubs/forum/01wforum/01wf175.pdf},
 volume = {Winter},
 year = {2001}
}

@article{Papush2021,
 author = {Papush, Dmitry E and Popelyukhin, Aleksey S and Zhang, Jasmine G},
 file = {:C\:/S/Library/Papush, Popelyukhin, Zhang/2021_Approximating the Aggregate Loss Distribution.pdf:pdf},
 journal = {Variance},
 keywords = {FFT,aggregate loss,aggregate models,collective risk model,compound distribution,gamma distribution,simulation},
 mendeley-tags = {FFT},
 number = {2},
 pages = {1--10},
 title = {{Approximating the Aggregate Loss Distribution}},
 volume = {14},
 year = {2021}
}

@book{Parodi2015,
 author = {Parodi, Pietro},
 file = {:C\:/S/Library/Parodi/2015_Pricing in General Insurance.pdf:pdf},
 isbn = {9781466581487},
 keywords = {Book},
 mendeley-tags = {Book},
 publisher = {CRC Press},
 title = {{Pricing in General Insurance}},
 year = {2015}
}

@article{Phillips1998,
 abstract = {This paper uses a contingent claims framework to develop a financial pricing model of insurance that overcomes one of the main shortcomings of previous models - the inability to price insurance by line in a multiple line insurer subject to default risk. The model predicts prices will vary across firms depending upon firm default risk, but within a given insurer prices should not vary after controlling for line-specific liability growth rates. We also analyze an important qualification to this result for insurance groups, where several insurer subsidiaries are owned by a primary insurer or holding company. Empirical tests using data on publicly traded property-liability insurers support the hypotheses: prices vary across firms depending upon overall-firm default risk and the concentration of business among subsidiaries; but within a given firm, prices do not vary by line after adjusting for line-specific liability growth rates.},
 annote = {From Duplicate 1 (Financial Pricing of Insurance in the Multiple-Line Insurance Company - Phillips, Richard D.; Cummins, J. David; Allen, Franklin)

From Duplicate 1 ( 

Financial Pricing of Insurance in the Multiple-Line Insurance Company

- Phillips, Richard D RD; Cummins, JD David; Allen, Franklin )




From Duplicate 2 ( 


Financial pricing of insurance in the multiple-line insurance company


- Phillips, RD; Cummins, JD; Allen, Franklin )



here are so e notes on this very important paper.









From Duplicate 2 ( 

Financial Pricing of Insurance in the Multiple-Line Insurance Company

- Phillips, Richard D RD; Cummins, JD David; Allen, Franklin )




From Duplicate 1 ( 


Financial Pricing of Insurance in the Multiple-Line Insurance Company


- Phillips, Richard D RD; Cummins, JD David; Allen, Franklin )




From Duplicate 2 ( 


Financial pricing of insurance in the multiple-line insurance company


- Phillips, RD; Cummins, JD; Allen, Franklin )



here are so e notes on this very important paper.









From Duplicate 2 ( 


Financial pricing of insurance in the multiple-line insurance company


- Phillips, RD; Cummins, JD; Allen, Franklin )



here are so e notes on this very important paper.},
 author = {Phillips, Richard D. and Cummins, J. David and Allen, Franklin},
 doi = {10.2307/253804},
 file = {:C\:/S/Library/Phillips, Cummins, Allen/1998_Financial Pricing of Insurance in the Multiple-Line Insurance Company(3).pdf:pdf},
 issn = {00224367},
 journal = {Journal of Risk and Insurance},
 keywords = {Cummins,Risk Measures},
 mendeley-tags = {Cummins,Risk Measures},
 number = {4},
 pages = {597--636},
 title = {{Financial Pricing of Insurance in the Multiple-Line Insurance Company}},
 url = {http://www.jstor.org/stable/253804 http://www.jstor.org/stable/10.2307/253804 http://www.jstor.org/stable/253804%5Cnhttp://www.jstor.org/stable/10.2307/253804},
 volume = {65},
 year = {1998}
}

@book{Press1992a,
 author = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
 edition = {2nd Editio},
 file = {:C\:/S/Library/Press et al/1992_Numerical Recipes in C.pdf:pdf},
 isbn = {0521431085},
 issn = {03088197},
 keywords = {Book},
 mendeley-tags = {Book},
 publisher = {Cambridge University Press},
 title = {{Numerical Recipes in C}},
 year = {1992}
}

@article{Puccetti2012,
 abstract = {We propose a new algorithm to compute numerically sharp lower and upper bounds on the distribution of a function of d dependent random variables having fixed marginal distributions. Compared to the existing literature, the bounds are widely applicable, more accurate and more easily obtained. ?? 2011 Elsevier B.V. All rights reserved.},
 author = {Puccetti, Giovanni and Ruschendorf, Ludger},
 doi = {10.1016/j.cam.2011.10.015},
 file = {:C\:/S/Library/Puccetti, Ruschendorf/2012_Computation of sharp bounds on the distribution of a function of dependent risks.pdf:pdf},
 issn = {03770427},
 journal = {Journal of Computational and Applied Mathematics},
 keywords = {Bounds for dependent risks,Distribution functions,Rearrangements,Risk Measures,Tails},
 mendeley-tags = {Risk Measures,Tails},
 number = {7},
 pages = {1833--1840},
 publisher = {Elsevier B.V.},
 title = {{Computation of sharp bounds on the distribution of a function of dependent risks}},
 url = {http://dx.doi.org/10.1016/j.cam.2011.10.015},
 volume = {236},
 year = {2012}
}

@article{Robertson1992,
 annote = {Reproduce his simple example

s = build('agg R1 5 claims sev chistogram xps [0 .2 .4 .6 .8 1] [.2 .2 .2 .2 .2] fixed'
, bs=1/200, log2=12)
s.density_df.loc[0:6:40, ['F']]},
 author = {Robertson, John P.},
 file = {:C\:/S/Library/Robertson/1992_The computation of aggregate loss distributions.pdf:pdf},
 journal = {Proceedings of the Casualty Actuarial Society},
 keywords = {FFT},
 mendeley-tags = {FFT},
 number = {150},
 pages = {57--133},
 title = {{The computation of aggregate loss distributions}},
 volume = {79},
 year = {1992}
}

@article{Rockafellar2014b,
 abstract = {Random variables can be described by their cumulative distribution functions, a class of nondecreasing functions on the real line. Those functions can in turn be identified, after the possible vertical gaps in their graphs are filled in, with maximal monotone relations. Such relations are known to be the subdifferentials of convex functions. Analysis of these connections yields new insights. The generalized inversion operation between distribution functions and quantile functions corresponds to graphical inversion of monotone relations. In subdifferential terms, it corresponds to passing to conjugate convex functions under the LegendreFenchel transform. Among other things, this shows that convergence in distribution for sequences of random variables is equivalent to graphical convergence of the monotone relations and epigraphical convergence of the associated convex functions. Measures of risk that employ quantiles (VaR) and superquantiles (CVaR), either individually or in mixtures, are illuminated in this way. Formulas for their calculation are seen from a perspective that reveals how they were discovered. The approach leads further to developments in which the superquantiles for a given distribution are interpreted as the quantiles for an overlying superdistribution. In this way a generalization of KoenkerBasset error is derived which lays a foundation for superquantile regression as a higher-order extension of quantile regression.  2014, Springer-Verlag Berlin Heidelberg and Mathematical Optimization Society (outside the USA).},
 author = {Rockafellar, R. T. and Royset, J. O.},
 doi = {10.1007/s10107-014-0801-1},
 file = {:C\:/S/Library/Rockafellar, Royset/2014_Random variables, monotone relations, and convex analysis.pdf:pdf},
 isbn = {1010701408011},
 issn = {14364646},
 journal = {Mathematical Programming},
 keywords = {Comonotonicity,Conditional-value-at-risk,Conjugate duality,Convergence in distribution,Convex analysis,Measures of risk,Quantiles,Random variables,Rockafellar,Stochastic dominance,Stochastic optimization,Superdistributions,Superexpectations,Superquantiles,Value-at-risk},
 mendeley-tags = {Rockafellar},
 number = {1-2},
 pages = {297--331},
 title = {{Random variables, monotone relations, and convex analysis}},
 volume = {148},
 year = {2014}
}

@article{Saumard2014,
 abstract = {We review and formulate results concerning log-concavity and strong-log-concavity in both discrete and continuous settings. We show how preservation of log-concavity and strong log-concavity on R under convolution follows from a fundamental monotonicity result of Efron (1965). We provide a new proof of Efron's theorem using the recent asymmetric Brascamp-Lieb inequality due to Otto and Menz (2013). Along the way we review connections between log-concavity and other areas of mathematics and statistics, including concentration of measure, log-Sobolev inequalities, convex geometry, MCMC algorithms, Laplace approximations, and machine learning.},
 archiveprefix = {arXiv},
 arxivid = {1404.5886},
 author = {Saumard, Adrien and Wellner, Jon A.},
 doi = {10.1214/14-SS107},
 eprint = {1404.5886},
 file = {:C\:/S/Library/Saumard, Wellner/2014_Log-concavity and strong log-concavity a review.pdf:pdf},
 issn = {19357516},
 journal = {Statistics Surveys},
 keywords = {Concave,Convex,Convolution,Inequalities,Log-concave,Monotone,Preservation,Strong log-concave,Tails},
 mendeley-tags = {Tails},
 pages = {45--114},
 title = {{Log-concavity and strong log-concavity: a review}},
 volume = {8},
 year = {2014}
}

@article{Schaller2008,
 author = {Schaller, P and Temnov, G},
 file = {:C\:/S/Library/Schaller, Temnov/2008_EFFICIENT AND PRECISE COMPUTATION OF CONVOLUTIONS APPLYING FFT TO HEAVY TAILED DISTRIBUTIONS.pdf:pdf},
 journal = {Computational Methods in Applied Mathematics},
 keywords = {1,FFT,characteristic functions,compound probability distributions,discrete,exponential window,fourier transform,introduction,loss aggregation is a,one needs to compute,the problem,usually,well-known problem in insurance,with},
 mendeley-tags = {FFT},
 number = {2},
 pages = {187--200},
 title = {{EFFICIENT AND PRECISE COMPUTATION OF CONVOLUTIONS: APPLYING FFT TO HEAVY TAILED DISTRIBUTIONS}},
 volume = {8},
 year = {2008}
}

@book{Shapiro2009,
 abstract = {The readers familiar with the area of optimization can easily name several classes of optimization problems, for which advanced theoretical results exist and efficient numerical\nmethods have been found. In that respect we can mention linear programming, quadratic programming, convex optimization, nonlinear optimization, etc. Stochastic programming sounds similar, but no specific formulation plays the role of the generic stochastic programming problem. The presence of random quantities in the model under consideration opens the door to wealth of different problem settings, reflecting different aspects of the applied problem at hand. The main purpose of this chapter is to illustrate the main approaches that can be followed when developing a suitable stochastic optimization model. For the purpose of presentation, these are very simplified versions of problems encountered in practice, but we hope that they will still help us to convey our main message.},
 annote = {From Duplicate 1 (Lectures on Stochastic Programming - Shapiro, Alexander; Dentcheva, Darinka; Ruszczy{\'{n}}ski, Andrzej)

Chapter 6: very nice intro to AVaR motivating why it is >= VaR (pp.262--)

Note (6.74) description of partial AVaR = measures such that AVaR = E_Q(X)... very nice!

Ex 6.18 = mean deviation risk measure order p
Ex 6.19 = mean upper semi deviation},
 archiveprefix = {arXiv},
 arxivid = {arXiv:1011.1669v3},
 author = {Shapiro, Alexander and Dentcheva, Darinka and Ruszczy{\'{n}}ski, Andrzej},
 doi = {10.1137/1.9780898718751},
 eprint = {arXiv:1011.1669v3},
 file = {:C\:/S/Library/Shapiro, Dentcheva, Ruszczy{\'{n}}ski/2009_Lectures on Stochastic Programming.pdf:pdf;:C\:/S/Library/Shapiro, Dentcheva, Ruszczy{\'{n}}ski/2009_Lectures on Stochastic Programming(2).pdf:pdf},
 isbn = {978-0-89871-687-0},
 issn = {01676377},
 keywords = {Book,Risk Measures},
 mendeley-tags = {Book,Risk Measures},
 number = {May},
 pmid = {15776329},
 publisher = {Society for Industrial and Applied Mathematics},
 title = {{Lectures on Stochastic Programming}},
 url = {http://epubs.siam.org/doi/book/10.1137/1.9780898718751},
 year = {2009}
}

@article{Sherris2006a,
 author = {Sherris, Michael},
 file = {:C\:/S/Library/Sherris/2006_Solvency, capital allocation, and fair rate of return in insurance.pdf:pdf;:C\:/S/Library/Sherris/2006_Solvency, capital allocation, and fair rate of return in insurance(2).pdf:pdf},
 journal = {Journal of Risk and Insurance},
 keywords = {Risk Measures},
 mendeley-tags = {Risk Measures},
 number = {1},
 pages = {71--96},
 title = {{Solvency, capital allocation, and fair rate of return in insurance}},
 url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2966.2006.00166.x/full},
 volume = {73},
 year = {2006}
}

@article{Shevchenko2010,
 abstract = {Estimation of the operational risk capital under the Loss Distribution Approach requires evaluation of aggregate (compound) loss distributions which is one of the classic problems in risk theory. Closed-form solutions are not available for the distributions typically used in operational risk. However with modern computer processing power, these distributions can be calculated virtually exactly using numerical methods. This paper reviews numerical algorithms that can be successfully used to calculate the aggregate loss distributions. In particular Monte Carlo, Panjer recursion and Fourier transformation methods are presented and compared. Also, several closed-form approximations based on moment matching and asymptotic result for heavy-tailed distributions are reviewed.},
 archiveprefix = {arXiv},
 arxivid = {1008.1108},
 author = {Shevchenko, Pavel V.},
 eprint = {1008.1108},
 file = {:C\:/S/Library/Shevchenko/2010_Calculation of aggregate loss distributions.pdf:pdf},
 journal = {Journal of Operational Risk},
 keywords = {FFT,aggregate loss distribution,carlo,compound distribution,fast fourier transform,loss distribution ap-,monte,operational risk,panjer recursion,proach},
 mendeley-tags = {FFT},
 number = {2},
 pages = {3--40},
 title = {{Calculation of aggregate loss distributions}},
 url = {http://arxiv.org/abs/1008.1108},
 volume = {5},
 year = {2010}
}

@book{Strain1997,
 author = {Strain, Robert W.},
 publisher = {Robert W. Strain Publishing & Seminars, Incorporated},
 title = {{Reinsurance}},
 year = {1997}
}

@article{Svindland2009,
 author = {Svindland, Gregor},
 doi = {10.1007/s11579-010-0026-x},
 file = {:C\:/S/Library/Svindland/2010_Continuity properties of law-invariant (quasi-)convex risk functions on {$Linfty$}.pdf:pdf;:C\:/S/Library/Svindland/2010_Continuity properties of law-invariant (quasi-)convex risk functions on {$Linfty$}.pdf:pdf},
 issn = {18629679},
 journal = {Mathematics and Financial Economics},
 keywords = {Duality,Fatou property,Law-invariant (quasi-)convex risk measures,Svindland},
 mendeley-tags = {Svindland},
 number = {1},
 pages = {39--43},
 title = {{Continuity properties of law-invariant (quasi-)convex risk functions on {$L^\infty$}}},
 volume = {3},
 year = {2010}
}

@article{Tasche1999,
 author = {Tasche, Dirk},
 file = {:C\:/S/Library/Tasche/1999_Risk contributions and performance measurement.pdf:pdf},
 journal = {Report of the Lehrstuhl fur mathematische Statistik, TU Munchen},
 keywords = {Risk Measures,capital asset pricing model,capm,performance measurement,portfolio selection,quan-,shortfall,tile,value at risk,var},
 mendeley-tags = {Risk Measures},
 pages = {1--26},
 title = {{Risk contributions and performance measurement}},
 url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.9393&rep=rep1&type=pdf},
 year = {1999}
}

@article{Temnov2008,
 author = {Temnov, Grigory and Warnung, Richard},
 file = {:C\:/S/Library/Temnov, Warnung/2008_A Comparison of Loss Aggregation Methods for Operational Risk.pdf:pdf},
 journal = {Journal of Operational Risk},
 keywords = {FFT,loss models,operational risk,recursive methods,risk aggregation,var},
 mendeley-tags = {FFT},
 number = {1},
 pages = {1--22},
 title = {{A Comparison of Loss Aggregation Methods for Operational Risk}},
 volume = {3},
 year = {2008}
}

@article{Tsanakas2003a,
 abstract = {The Aumann-Shapley [Values of Non-atomic Games, Princeton University Press, Princeton] value, originating in cooperative game theory, is used for the allocation of risk capital to portfolios of pooled liabilities, as proposed by Denault [Coherent allocation of risk capital, J. Risk 4 (1) (2001) 1]. We obtain an explicit formula for the Aumann-Shapley value, when the risk measure is given by a distortion premium principle [Axiomatic characterisation of insurance prices, Insur. Math. Econ. 21 (2) (1997) 173]. The capital allocated to each instrument or (sub)portfolio is given as its expected value under a change of probability measure. Motivated by Mirman and Tauman [Demand compatible equitable cost sharing prices, Math. Oper. Res. 7 (1) (1982) 40], we discuss the role of Aumann-Shapley prices in an equilibrium context and present a simple numerical example. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
 annote = {Very nice. Digest.},
 author = {Tsanakas, Andreas and Barnett, Christopher},
 doi = {10.1016/S0167-6687(03)00137-9},
 file = {:C\:/S/Library/Tsanakas, Barnett/2003_Risk capital allocation and cooperative pricing of insurance liabilities.pdf:pdf},
 isbn = {0167-6687, 0167-6687},
 issn = {01676687},
 journal = {Insurance: Mathematics and Economics},
 keywords = {Aumann-Shapley value,Coherent risk measures,Cooperative games,Distortion premium principle,Equilibrium,Risk Measures,Tsanakas},
 mendeley-tags = {Risk Measures,Tsanakas},
 number = {2},
 pages = {239--254},
 title = {{Risk capital allocation and cooperative pricing of insurance liabilities}},
 volume = {33},
 year = {2003}
}

@article{Venter2006,
 abstract = {Only 16 citations!},
 author = {Venter, Gary G. and Major, John A. and Kreps, Rodney E.},
 doi = {10.2143/AST.36.2.2017927},
 file = {:C\:/S/Library/Venter, Major, Kreps/2006_Marginal Decomposition of Risk Measures.pdf:pdf},
 issn = {0515-0361},
 journal = {ASTIN Bulletin},
 keywords = {GSC,Major,Risk Measures,Venter},
 mendeley-tags = {GSC,Major,Risk Measures,Venter},
 month = {oct},
 number = {2},
 pages = {375--413},
 title = {{Marginal Decomposition of Risk Measures}},
 url = {http://poj.peeters-leuven.be/content.php?url=article&id=2017927},
 volume = {36},
 year = {2006}
}

@article{Verrall2004,
 author = {Verrall, R J},
 file = {:C\:/S/Library/Verrall/2004_Bayesian Generalized Linear Model for the Bornhuetter-Furguson Method of Claims Reserving.pdf:pdf},
 journal = {North American Actuarial Journal},
 number = {3},
 pages = {67--89},
 title = {{Bayesian Generalized Linear Model for the Bornhuetter-Furguson Method of Claims Reserving}},
 volume = {8},
 year = {2004}
}

@article{Vitale1990,
 author = {Vitale, Richard A.},
 chapter = {On stochas},
 isbn = {978-0-940600-23-2},
 journal = {Lecture Notes-Monograph Series},
 pages = {459--469},
 publisher = {Institute of Mathematical Statistics, Hayward CA},
 series = {In Topics in Statistical Dependence, Edited by H. Block, A. Sampson and T. Savits},
 title = {{On stochastic dependence and a class of degenerate distributions}},
 url = {http://projecteuclid.org/euclid.lnms/1215457581},
 year = {1990}
}

@article{Wang1995,
 abstract = {This paper proposes a new premium principle, where risk loadings are imposed by a proportional decrease in the hazard rates. This premium principle is scale invariant and additive for layers. It is shown that this principle will generate stop-loss contracts as optimal reinsurance arrangements in a competitive market when the reinsurer is less risk-averse than the direct insurer. Finally, increased limits factors are calculated based on this principle. {\textcopyright} 1995, All rights reserved.},
 author = {Wang, Shaun S.},
 doi = {10.1016/0167-6687(95)00010-P},
 file = {:C\:/S/Library/Wang/1995_Insurance pricing and increased limits ratemaking by proportional hazards transforms.pdf:pdf},
 issn = {01676687},
 journal = {Insurance: Mathematics and Economics},
 keywords = {Increased limits factors,Optimal reinsurance,Premium principle,Proportional hazards transform,Risk-averse,WangSS},
 mendeley-tags = {WangSS},
 number = {1},
 pages = {43--54},
 publisher = {Elsevier Science B.V.},
 title = {{Insurance pricing and increased limits ratemaking by proportional hazards transforms}},
 url = {http://dx.doi.org/10.1016/0167-6687(95)00010-P},
 volume = {17},
 year = {1995}
}

@article{Wang1996,
 abstract = {500-1000 citations},
 author = {Wang, Shaun S.},
 doi = {10.2143/AST.26.1.563234},
 file = {:C\:/S/Library/Wang/1996_Premium Calculation by Transforming the Layer Premium Density.pdf:pdf},
 isbn = {1783-1350},
 issn = {0515-0361},
 journal = {ASTIN Bulletin},
 keywords = {1,GSC,WangSS,comonotomcxty,i n t r,iance analysis,mean-var-,o d u c,premmm calculation principle,proportional hazard transform,stochastic dominance,t i o n},
 mendeley-tags = {GSC,WangSS},
 number = {01},
 pages = {71--92},
 title = {{Premium Calculation by Transforming the Layer Premium Density}},
 url = {https://www.cambridge.org/core/product/identifier/S0515036100003214/type/journal_article},
 volume = {26},
 year = {1996}
}

@article{Wang2000,
 abstract = {This article introduces a class of distortion operators, g(alpha)(u) =\nPhi{[}Phi(-1)(u) + alpha], where Phi is the standard normal cumulative\ndistribution. For any loss (or asset) variable X with a probability\ndistribution S-x(x) = 1-F-x(x), g(alpha){[}S-x(x)] defines a distorted\nprobability distribution whose mean value yields a risk-adjusted premium\n(or an asset price). The distortion operator g(alpha) can be applied to\nboth assets and liabilities, with opposite signs in the parameter alpha.\nBased on CAPM, the author establishes that the parameter alpha should\ncorrespond to the systematic risk of X. For a normal (mu, sigma(2))\ndistribution, the distorted distribution is also normal with mu' = mu +\nalpha sigma and sigma' = sigma. For a lognormal distribution, the\ndistorted distribution is also lognormal. By applying the distortion\noperator to stock price distributions, the author recovers the\nrisk-neutral valuation for options and in particular the Black-Scholes\nformula.},
 author = {Wang, Shaun S.},
 doi = {10.2307/253675},
 file = {:C\:/S/Library/Wang/2000_A Class of Distortion Operators for Pricing Financial and Insurance Risks.pdf:pdf},
 issn = {00224367},
 journal = {The Journal of Risk and Insurance},
 keywords = {WangSS},
 mendeley-tags = {WangSS},
 number = {1},
 pages = {15--36},
 title = {{A Class of Distortion Operators for Pricing Financial and Insurance Risks}},
 volume = {67},
 year = {2000}
}

@article{Wang2002,
 abstract = {This article introduces a class of distortion operators, ga(t) = D[44-(u) + a], where D is the standard normal cumulative distribution. For any loss (or asset) variable X with a probability distribution Sx(x) = 1- Fx(x), ga [Sx(x)] defines a distorted probability distribution whose mean value yields a risk-adjusted premium (or an asset price). The distortion operator ga can be applied to both assets and liabilities, with opposite signs in the parameter a. Based on CAPM, the author establishes that the parameter ca should correspond to the systematic risk of X. For a normal (L,aU2) distribution, the distorted distribution is also normal with '= u + aa and a5' = a. For a lognormal distribution, the distorted distribution is also lognormal. By applying the distortion operator to stock price distributions, the author recovers the risk-neutral valuation for options and in particular the Black-Scholes formula},
 author = {Wang, Shaun S.},
 doi = {10.2143/AST.32.2.1027},
 file = {:C\:/S/Library/Wang/2002_A Universal Framework for Pricing Financial and Insurance Risks.pdf:pdf},
 isbn = {1783-1350},
 issn = {05150361},
 journal = {ASTIN Bulletin},
 keywords = {WangSS},
 mendeley-tags = {WangSS},
 number = {2},
 pages = {213--234},
 title = {{A Universal Framework for Pricing Financial and Insurance Risks}},
 volume = {32},
 year = {2002}
}

@article{WangS1998,
 abstract = {This paperpresentsa set of tools formodeling and combining correlated risks. Various correlation struc- tures are generated using copula, common mixture, com- ponent, and distortion models. These correlation struc- tures are specified in terms of (i) the joint cumulative distribution function or (ii) the joint characteristic func- tion and lend themselves to efficient methods of aggre- gation by using Monte Carlo simulation or fast Fourier transform.},
 author = {Wang, Shaun S.},
 file = {:C\:/S/Library/Wang/1998_Aggregation of correlated risk portfolios models and algorithms.pdf:pdf},
 journal = {Proceedings of the Casualty Actuarial society},
 keywords = {FFT,WangSS},
 mendeley-tags = {FFT,WangSS},
 pages = {848--939},
 title = {{Aggregation of correlated risk portfolios: models and algorithms}},
 url = {http://www.casact.com/pubs/proceed/proceed98/980848.pdf},
 year = {1998}
}

@article{Wilson2016,
 abstract = {A novel method is presented for fast convolution of a pair of probability mass functions defined on a finite lattice with guaranteed accuracy of all computed values. This method, called aFFT-C (accurate FFT convolution), utilizes the Fast Fourier Transform (FFT) for the gain in speed, but relying on a rigorous analysis of the propagation of roundoff error, it can detect and circumvent the accumulation of this numerical error that is otherwise inherent to the Fourier transform. In the worst case scenario aFFT-C's execution time is on par with the accurate naive convolution but in a typical application it is comparable with a direct FFT-based convolution (FFT-C). The properties of the proposed algorithm are demonstrated both theoretically and empirically.},
 author = {Wilson, Huon and Keich, Uri},
 doi = {10.1016/j.csda.2016.03.010},
 file = {:C\:/S/Library/Wilson, Keich/2016_Accurate pairwise convolutions of non-negative vectors via FFT.pdf:pdf},
 issn = {01679473},
 journal = {Computational Statistics and Data Analysis},
 keywords = {Accuracy-speed tradeoff,Convolution,Distribution on a lattice,FFT,Fast Fourier transform,Roundoff error},
 mendeley-tags = {FFT},
 pages = {300--315},
 publisher = {Elsevier B.V.},
 title = {{Accurate pairwise convolutions of non-negative vectors via FFT}},
 url = {http://dx.doi.org/10.1016/j.csda.2016.03.010},
 volume = {101},
 year = {2016}
}

@article{Woo2002,
 abstract = {The procedure for estimating probable maximum loss (PML) for natural catastrophes has evolved over the past few decades from a rather simplistic deterministic basis to a more sophisticated methodology based on loss exceedance probability curves, generated using catastrophe modelling software. This development process is reviewed, with an emphasis on the earthquake peril, which, because of its widespread threat to critical industrial installations, has been at the forefront of most PML advances. The coherent risk definition of PML is advocated as an improvement over standard quantile methods, which can give rise to anomalous aggregation results failing to satisfy the fundamental axiom of subadditivity, and so discouraging the pooling of risks.},
 author = {Woo, G.},
 doi = {10.1017/s1357321700004037},
 file = {:C\:/S/Library/Woo/2002_Natural Catastrophe Probable Maximum Loss.pdf:pdf},
 issn = {1357-3217},
 journal = {British Actuarial Journal},
 number = {5},
 pages = {943--959},
 title = {{Natural Catastrophe Probable Maximum Loss}},
 volume = {8},
 year = {2002}
}
